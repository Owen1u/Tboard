python train/process.py train_vocab --vocab_size 50000 --filename data/reddit-amazon-5m.txt --defined_tokens tokenizer/core.txt